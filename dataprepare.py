# -*- coding: utf-8 -*-
"""DataPrepare.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ChODp3S57AL9izZogBeI70GnQ1VqS9Bk

# Retrieve Data

I was able to download four files from PGE containing 60 minute interval data on my electricity usage. Let's merge the files and prepare the data for modeling.
"""

# Import the necessary libraries
import sys
import os
import pandas as pd
from config import RAW_DATA_PATH, PROCESSED_DATA_PATH, TRAINED_MODEL_PATH
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

raw_data_file = os.path.join(RAW_DATA_PATH, "usage_data.csv")
merged_df = pd.read_csv(raw_data_file)

"""Now let's clean the data by removing some unnecessary attributes

"""

merged_df.drop(['TYPE','NOTES','COST'], axis = 1, inplace=True)
print(merged_df.head())

print(merged_df.info())

"""# Feature Engineering"""

merged_df["datetime"] = pd.to_datetime(merged_df['DATE'] + ' ' + merged_df['START TIME'])
merged_df['usage_kWh'] = merged_df['USAGE (kWh)']
energy_data = merged_df[['datetime', 'usage_kWh']]

energy_data.loc[:,'hour'] = energy_data['datetime'].dt.hour
energy_data.loc[:,'weekday'] = energy_data['datetime'].dt.weekday
energy_data.loc[:,'month'] = energy_data['datetime'].dt.month

energy_data.head()

def time_feature_engr(energy_data):
  #Cyclic encoding for periodic features
  #This maps the hour, weekday, and month features so that the distance between values is consistent
  energy_data.loc[:,"weekday_sin"] = np.sin(2 * np.pi * energy_data["weekday"] / (7*24))
  energy_data.loc[:,"weekday_cos"] = np.cos(2 * np.pi * energy_data["weekday"] / (7*24))
  energy_data.loc[:,"hour_sin"] = np.sin(2 * np.pi * energy_data["hour"] / 24)
  energy_data.loc[:,"hour_cos"] = np.cos(2 * np.pi * energy_data["hour"] / 24)
  energy_data.loc[:,"month_sin"] = np.sin(2 * np.pi * energy_data["month"] / (30.4*24))
  energy_data.loc[:,"month_cos"] = np.cos(2 * np.pi * energy_data["month"] / (30.4*24))

  #Include a weekday-hour interaction to model time-based patterns
  #This could caputre peak usage, say on Friday nights or Saturday mornings
  energy_data["weekday_hour"] = energy_data.weekday.astype(str) + '-' + energy_data.hour.astype(str)

  #weekday_hour will be inluded as an object variable so we need to transform it using
  #one-hot encoding or lebel encoding. I'll use label encoding becasue it's less
  #computationally expensive. Now,  each weekday-hour combination will have its own label
  from sklearn.preprocessing import LabelEncoder
  le = LabelEncoder()
  energy_data["weekday_hour"] = le.fit_transform(energy_data["weekday_hour"])
  return energy_data

energy_data = time_feature_engr(energy_data)
energy_data.head()

# Find the minimum and maximum values so the corresponding weather data can be retreived
min_datetime = energy_data['datetime'].min()
max_datetime = energy_data['datetime'].max()

# Print the results
print("Minimum datetime:", min_datetime)
print("Maximum datetime:", max_datetime)

#Sort the data to make sure the temporal integrity is maintained in the data
energy_data = energy_data.sort_values(by = 'datetime')
energy_data.reset_index(drop = True, inplace = True)

# Read in measured weather data
weather_data_path = os.path.join(RAW_DATA_PATH, "measured_weather_data.csv")
weather_data = pd.read_csv(weather_data_path)
weather_data.head()

# Clean the weather data and merge it with the dataset
weather_data_clean = weather_data.drop(columns = ['source'])
weather_data_clean['datetime'] = pd.to_datetime(weather_data_clean['datetime'])
weather_data_clean.set_index('datetime', inplace=True)

dataset = energy_data.merge(weather_data_clean, on = 'datetime', how = 'left')
col = dataset.pop('usage_kWh')
dataset.insert(dataset.shape[1], 'usage_kWh', col)
print(dataset.head())

#Apply a log transformation to target variable to help model deal with variance in the data
dataset.loc[:, "usage_kWh"] = np.log1p(dataset["usage_kWh"])

#This function adds a lag feature based on past values only. This would make sense for
# models used for both forecasting and estimating past values. Using a centered lag could be suitable
#for estimating past values only because future values are not available in forecasting scenarios.

def add_lag_feature(df, window, shift = 1, stat = ['mean','max','min','std'], lag_cols = ["temp","solarradtion","cloudcover"]):

  if stat == 'mean':
    for col in lag_cols:
        df[f"{col}_mean_lag{window}_sh{shift}"] = (df[col]
            .shift(shift)  # Shift by 1 to exclude the current value
            .rolling(window=window, min_periods= window)  # Rolling over past values
            .mean() # Take the mean of the previous three values
        )
  elif stat == 'max':
    for col in lag_cols:
        df[f"{col}_max_lag{window}_sh{shift}"] = (df[col]
        .shift(shift)
        .rolling(window=window, min_periods= window)
        .quantile(0.95) # Take the 95th percentile of the previous three values
        )
  elif stat == 'min':
    for col in lag_cols:
        df[f"{col}_min_lag{window}_sh{shift}"] = (df[col]
        .shift(shift)
        .rolling(window=window, min_periods= window)
        .quantile(0.05) # Take the 5th percentile of the previous three values
        )
  elif stat == 'std':
    for col in lag_cols:
        df[f"{col}_std_lag{window}_sh{shift}"] = (df[col]
        .shift(shift)
        .rolling(window=window, min_periods= window)
        .std() # Take the standard devition of the previous three values
        )


add_lag_feature(dataset, window = 3, stat = 'mean', lag_cols = ["temp"] )
add_lag_feature(dataset, window = 7, stat = 'mean', lag_cols = ["temp"] )
add_lag_feature(dataset, window = 24, stat = 'mean', lag_cols = ["temp"] )
add_lag_feature(dataset, window = 3, stat = 'max', lag_cols = ["temp"] )
add_lag_feature(dataset, window = 3, stat = 'min', lag_cols = ["temp"] )
add_lag_feature(dataset, window = 3, stat = 'std', lag_cols = ["temp"] )
add_lag_feature(dataset, window = 3, stat = 'mean', lag_cols = ["solarradiation"] )
add_lag_feature(dataset, window = 24, stat = 'mean', lag_cols = ["solarradiation"] )
add_lag_feature(dataset, window = 3, stat = 'mean', lag_cols = ["cloudcover"] )

add_lag_feature(dataset, window = 3, shift = 24, stat = 'mean', lag_cols = ["temp"] )

col = dataset.pop('usage_kWh')
dataset.insert(dataset.shape[1], 'usage_kWh', col)

"""# EDA
Let's do some visualizations to get an idea of what the data looks like.
"""

#Line plot for electricity usage and temperature
plt.figure(figsize = (12,6))
plt.plot(dataset['datetime'], dataset['usage_kWh'])
plt.xlabel('Date')
plt.ylabel('Electricity Usage (kWh)')
plt.title('Electricity Usage Over Time')
plt.show()

"""
Focus the plot on a subset of data to generate more a interpretable and visualizations."""

# Filter the dataset for values from May 2024
# Let's also overlay temperature while normalizing the data
dataset_may_2024 = dataset[(dataset['datetime'].dt.year == 2024) & (dataset['month'] == 5)]

plt.figure(figsize = (12,6))
plt.plot(dataset_may_2024['datetime'], dataset_may_2024['usage_kWh']/dataset_may_2024['usage_kWh'].max(), label = 'Electricity Usage')
plt.plot(dataset_may_2024['datetime'], dataset_may_2024['temp']/dataset_may_2024['temp'].max(), label = 'Temperature')
# plt.plot(dataset_may_2024['datetime'], dataset_may_2024['cloudcover']/dataset_may_2024['cloudcover'].max(), label = 'Cloud cover')
plt.xlabel('Date')
plt.ylabel('Normalized Value')
plt.title('Energy and Temperature Over Time')
plt.legend(labels=['kWh','temp'])
plt.show()

"""Let's take a closer look at the beginning of May to see if we can visualize why the temperature dipped."""

# Filter the dataset for values from May 2024

dataset_may_dates = dataset[(dataset['datetime'].dt.year == 2024) & (dataset['month'] == 5) & (dataset['datetime'].dt.day <= 6)]

plt.figure(figsize = (12,6))
plt.plot(dataset_may_dates['datetime'], dataset_may_dates['usage_kWh']/dataset_may_dates['usage_kWh'].max(), label = 'Electricity Usage')
plt.plot(dataset_may_dates['datetime'], dataset_may_dates['temp']/dataset_may_dates['temp'].max(), label = 'Temperature')
plt.plot(dataset_may_dates['datetime'], dataset_may_dates['cloudcover']/dataset_may_dates['cloudcover'].max(), label = 'Cloud cover')
plt.xlabel('Date')
plt.ylabel('Normalized Value')
plt.title('Energy and Temperature Over Time')
plt.legend(labels=['kWh','temp', 'cloudcover'])
plt.show()

"""#Initial Feature Selection
Let's drop some features because they are either highly correlated with each other or redundant. Use domain knowledge to drop these features because including them can introduce multicollinearity, inflate the model's complexity, and dilute the predictive power of truly informative features, potentially leading to overfitting.
"""

correlation_matrix = dataset.drop(columns=['datetime']).corr()

# Identify highly correlated features
threshold = 0.8  # You can adjust this threshold
high_correlation_pairs = []

# Iterate over the upper triangle of the matrix (to avoid duplicates)
for i in range(len(correlation_matrix.columns)):
    for j in range(i + 1, len(correlation_matrix.columns)):
        if abs(correlation_matrix.iloc[i, j]) > threshold:
            feature_1 = correlation_matrix.columns[i]
            feature_2 = correlation_matrix.columns[j]
            high_correlation_pairs.append((feature_1, feature_2, correlation_matrix.iloc[i, j]))

plt.figure(figsize=(12, 10))  # Adjust the figure size as needed
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", cbar=True, annot_kws={"size": 8})
plt.title("Feature Correlation Heatmap", fontsize=16)
plt.xticks(rotation=45, ha='right', fontsize=10)  # Rotate x-axis labels for better readability
plt.yticks(fontsize=10)
plt.tight_layout()
plt.show()

#Drop the 'datetime' feature becasue it's an object type and we already respresent this value with other features
dataset.drop(columns = ['datetime'], inplace = True)

#Drop 7-hour and 24-hr temp lag features - they have lower correlation to the target than the 3-hr lag
#larger lag features may make sense for builings with higher thermal mass
dataset.drop(columns = ['temp_mean_lag7_sh1','temp_mean_lag24_sh1'], inplace = True)

#Drop month feature - I don't believe any certain month to be associated with higher energy use
dataset.drop(columns = ['month'], inplace = True)

#Drop solar radiation 24-hr lag becasue it is highly correlated with all of the temperature features
#Keep solar radiation 3-hr lag because it seems to have less overlap with temp variables, has a higher
#correlation to the target, and makes sense that the 3-hr lag would contribute more to energy use
dataset.drop(columns = ['solarradiation_mean_lag24_sh1'], inplace = True)

#Keep max, min, standard deviation lags even though they are highly correlated to other features
#becasue I want to capture the effects of spiking, dipping, or variable temps. Perhaps RCVFE will
#eliminate these

#Considered dropping current solar radiation and cloud cover features becasue these likely don't
#affect the target variable within an hour, but will keep them in and see if RCVFE eliminates them
# dataset.drop(columns = ['temp','solarradiation','cloudcover'], inplace = True)

#Determine how to split the dataset into training and test sets.
#There's a little more than 3 years worth of data. Let's use the first 2 years to train
#and the last year as the test set

total_years = len(dataset)/(24*365)
test_years = 1
train_years = total_years - test_years

train_observations = int(train_years * (24 * 365))
test_observations = int(test_years * (24 * 365))

train_perc = train_observations/len(dataset)
test_perc = test_observations/len(dataset)

#Make sure I got the calculation right
print(train_perc)
print(test_perc)
print(train_perc + test_perc)


train_df = dataset.iloc[:train_observations,:]
test_df = dataset.iloc[train_observations:,:]

#Save the training dataset at a pickle file
root = "/content/drive/MyDrive/DTSA 5509 - Intro to ML/Home_Energy_Project/processed_data"
output_path = os.path.join(PROCESSED_DATA_PATH, "processed_training_data.pkl")
train_df.to_pickle(output_path)

#Save the testing dataset at a pickle file
root = "/content/drive/MyDrive/DTSA 5509 - Intro to ML/Home_Energy_Project/processed_data"
output_path = os.path.join(PROCESSED_DATA_PATH, "processed_test_data.pkl")
test_df.to_pickle(output_path)

test_df.head()